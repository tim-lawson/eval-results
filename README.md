# Evaluation Results Parsers

This repository provides simple scripts to parse evaluation results from language model evaluation harnesses, making it easier to collate results for charts and tables.

The primary purpose of these scripts is to find all evaluation result files within a specified directory, parse them, and combine them into a single CSV file. This is particularly useful when you have trained and evaluated multiple models and need to compare their performance.

The scripts are designed to work with the output from the following popular evaluation harnesses:

- [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [ZubinGou/math-evaluation-harness](https://github.com/ZubinGou/math-evaluation-harness)

## Scripts

- `lm_eval_results.py`: Parses results from `lm-evaluation-harness`. It recursively finds all `results*\*.json` files in a directory, extracts the metrics, and collates them into a single DataFrame.

- `math_eval_results.py`: Parses results from `math-evaluation-harness`. It finds all `*.json` files within `math*eval*` subdirectories, extracts the accuracy scores, and pivots the data to create a summary DataFrame.

## Usage

Clone the repository.
Create a Python virtual environment and install dependencies. I recommend [uv](https://github.com/astral-sh/uv).
Run the scripts, pointing them to the directory containing your evaluation results. By default, they look for an output directory.

```sh
python lm_eval_results.py
python math_eval_results.py
```

The scripts will save the parsed results to `lm_eval_results.csv` and `math_eval_results.csv` respectively.

This README was generated by Gemini 2.5 Pro.
